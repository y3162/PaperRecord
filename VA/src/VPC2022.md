[戻る](../list.md)
# まとめ
VoicePrvacyChallenge2022の結果と分析についてのレビュー.

## 音声プライバシー保護の現状
声の音色や性などの個人情報などの秘匿化が求められているが、このコミュニティは匿名化に注力している.

## データセット
![](VPC2020-Dataset-model.png)
モデルの訓練・評価に使うデータセット.

![](VPC2020-Dataset-enrollment.png)
攻撃モデルに使うデータセット.

## 評価指標
### A.新たな客観的指標
VPC2020では、参加者はプライバシーと実用性の適切なトレードオフを自分で定める必要があり、それぞれが異なる指標で最適化を試みていた.
そこでVPC2022では、徐々に上昇するプライバシー条件(EER)を満たしたシステムが実用性(WER)についてランク付けされる.
15%,20%,25%,30%の段階的な条件を満たすConditionに提出することができる.
EER、WERは評価用データセットの重み付き(VPC2022では等重み付き)で平均で計算される.

### B.他の客観的指標
#### 1.ピッチ相関 $\rho^{F_{0}}$
ピッチの相関係数.
各データセットで閾値を超える必要があり、ベースラインをもとに、$\rho^{F_{0}}>0.3$が要求される.

#### 2.音声の独自性 $G_{\text{VD}}$
$$
M\left(i,j\right)=\text{sigmoid}\left(\frac{1}{n_{i}n_{j}}\sum_{\substack{1\leq k\leq n_{i}\\1\leq\ell\leq n_{j}\\k\neq\ell\text{ if }i=j}}\text{LLR}\left(x_{k}^{\left(i\right)},x_{\ell}^{\left(j\right)}\right)\right)
$$
$x_{k}^{\left(i\right)}$は発話$k$が話者$i$によるものである確率である.
つまり、$M\left(i,j\right)$は、話者$i$と$j$の類似度を表す指標だと考えられる.

$M$の対角成分の相対的な大きさは、
$$
D_{\text{diag}}\left(M\right)=\left|\sum_{1\leq i\leq N}\frac{M\left(i,i\right)}{N}-\sum_{\substack{1\leq j\leq N\text{ and }1\leq k\leq N\\j\neq k}}\frac{M\left(j,k\right)}{N\left(N-1\right)}\right|
$$
であり、話者認証の精度を表す指標である.

そして、2つのシチュエーションにおけるこの指標の相対的な評価は以下で与えられる.
$$
G_{\text{VD}}=10\log_{10}\frac{D_{\text{diag}}\left(M_{aa}\right)}{D_{\text{diag}}\left(M_{oo}\right)}
$$
$G_{\text{VD}}$が0と比べて大きいほど、精度が元よりも向上したことを表す.

## 匿名化システム
### ベースライン
#### ベースライン$\mathbf{B1}$
![](B1.ab.png)
##### ベースライン$\mathbf{B1.a}$
VPC2020bの$\mathbf{B1}$と同じである.
##### ベースライン$\mathbf{B1.b}$
$\mathbf{B1.a}$の音声合成のモジュールを[HiFi-GAN](../../TTS/src/HiFi-GAN.md)をベースにしたシステムに置き換えたもの.
#### ベースライン$\mathbf{B2}$
VPC2020の$\mathbf{B2}$とほぼ同じ.
MaAdams係数$\alpha$を$\left(\alpha_{\min},\alpha_{\max}\right)$の範囲から選ぶことが相違点.

### 提出されたシステム
各システムは\<team number>-\<'p' if primary 'c' if contrasive>-\<incremental identifier>のフォーマットで表される.
### Team $\mathbf{T04}$
![](VPC2022-T04.png)
#### Speech Recognition Module
- [Hybrid CTC/Attension ASR モデル](../../ASR/src/Hybird-CTCAttension.md)[ESPnet2 toolkit]を用いて音声データをテキストデータへ変換する.
- エンコーダーはConformerで、デコーダーはTransformerである.
- 100ユニグラム言語モデルを仮定し、音素単位に分割した[IMS Toucan toolkit]文字起こしを用いてSεntencePieceを学習した.
- データ拡張(SpecAugument,3-way speed perturbation)をしたデータで学習を行なった.

#### Speaker Embedding Extraction Module
- ECAPA-TDNNの中間表現192次元とx-vector512次元を結合した、704次元の話者埋め込みを用いる.

#### Speaker Embedding Anonymization Module
- 人工的な話者埋め込みを生成するために、WGAN-QCを用いる.
- WGANは、GANのDiscriminatorではなく、Criticという本物と偽物の分布の距離を縮めるネットワークを用いる.
- GeneratorおよびCriticは簡略化されたResNetを用いて実装されている.
- ResNetの代わりに同じパラメータ数のMLPを用いたところ、学習の収束が遅くなった.
- 匿名化では元の話者の情報が一切使用されない.男女それぞれの声を生成するGANの構築を試みたが、データが十分でなかったため、1つのGANを用いる.

#### Speech Synthesis Module
- TTSモジュールは FastSpeech 2 で、IMS Toucan toolkit によって実装され、学習データには LibriTTS-clean-100 を使用した.
- FastSpeech 2 は、堅牢性と速度に重点を置いた音声合成のモデルである.
- Encoder および　Decoder には Conformer を用いた.
